{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sagemaker import Session\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Foundation Model based Applications\n",
    "\n",
    "Working with LLMs to provide you with advanced reasoning and routing capabilities is easy to get started with. After all the models understand human level instructions, and provide formattable string outputs as a result. \n",
    "\n",
    "Yet, when you are looking to develop production ready applications you will require robust data integrations to provide input to your model, you want to solve the alignment problem with LLMs, tune the behavior to your specific corporate governance and brand messaging.\n",
    "\n",
    "Complex workflows will involve multiple stages to create intermediate results. These stages require the model to switch roles, or might involve optimized task models to be more efficient, or even fine-tuned further on the specific tasks. \n",
    "\n",
    "All of this creates complexity when creating and maintaining your LLM based applications in practice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredToken) when calling the GetCallerIdentity operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Initialize key environment variables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sagemaker_session \u001b[39m=\u001b[39m Session()\n\u001b[0;32m----> 6\u001b[0m aws_role \u001b[39m=\u001b[39m sagemaker_session\u001b[39m.\u001b[39;49mget_caller_identity_arn()\n\u001b[1;32m      7\u001b[0m aws_region \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mSession()\u001b[39m.\u001b[39mregion_name\n\u001b[1;32m      8\u001b[0m sm_client \u001b[39m=\u001b[39m boto3\u001b[39m.\u001b[39mclient(\u001b[39m\"\u001b[39m\u001b[39msagemaker\u001b[39m\u001b[39m\"\u001b[39m, aws_region)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/sagemaker/session.py:4706\u001b[0m, in \u001b[0;36mSession.get_caller_identity_arn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4695\u001b[0m     \u001b[39mexcept\u001b[39;00m ClientError:\n\u001b[1;32m   4696\u001b[0m         LOGGER\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   4697\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdescribe_notebook_instance\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to get the Role \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4698\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mARN of the instance \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   4699\u001b[0m             instance_name,\n\u001b[1;32m   4700\u001b[0m         )\n\u001b[1;32m   4702\u001b[0m assumed_role \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session\u001b[39m.\u001b[39;49mclient(\n\u001b[1;32m   4703\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39msts\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   4704\u001b[0m     region_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_region_name,\n\u001b[1;32m   4705\u001b[0m     endpoint_url\u001b[39m=\u001b[39;49msts_regional_endpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_region_name),\n\u001b[0;32m-> 4706\u001b[0m )\u001b[39m.\u001b[39;49mget_caller_identity()[\u001b[39m\"\u001b[39m\u001b[39mArn\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   4708\u001b[0m role \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m^(.+)sts::(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+):assumed-role/(.+?)/.*$\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1iam::\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m2:role/\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m3\u001b[39m\u001b[39m\"\u001b[39m, assumed_role)\n\u001b[1;32m   4710\u001b[0m \u001b[39m# Call IAM to get the role's path\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the GetCallerIdentity operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "#load stored variables from previous notebook\n",
    "%store -r\n",
    "\n",
    "# Initialize key environment variables\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "model_version = \"*\"\n",
    "\n",
    "print(inference_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering your Third-Party API key (PURELY OPTIONAL!)\n",
    "We will be running a section on the ChatModel API exposed by a series of API endpoint providers such as OpenAI, Anthropic, Google Vertex. As this is currently not supported by the SageMaker deployed models, you can choose to experimment with at your own costs if you register for an OpenAI key, or you have previous access to Anthropic (as they are currently not accepting new registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key=\"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\"\n",
    "anthropic_api_key=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If you have an api key for OpenAI\n",
    "import sys\n",
    "if openai_api_key:\n",
    "    !{sys.executable} -m pip install openai\n",
    "if anthropic_api_key:\n",
    "    !{sys.executable} -m pip install anthropic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Widgets used across the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Select, Text\n",
    "\n",
    "# This creates the widgets used across the notebook for easier configuration\n",
    "models = ['SageMaker-Falcon40B']\n",
    "# Subset based on available ApiKeys\n",
    "if openai_api_key:\n",
    "    models.append('OpenAI')\n",
    "if anthropic_api_key:\n",
    "    models.append('Anthropic-Claude')\n",
    "\n",
    "model_selection = Select(\n",
    "    options=models\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the power of LangChain\n",
    "Recently the community unified their efforts on a high-level Framework to ease the development of foundation model based applications.\n",
    "LangChain was developed to ease the integration of models deployed, or used over proprietary APIs. It lets you easily integrate models into your application, manage the templates for prompts to tune your model behaviour, provide IO, add memory and chain multiple reasoning and action steps. \n",
    "\n",
    "### What is LangChain\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "It helps us with:\n",
    "1. **Integration** - Bring external data, such files, databases, webcontent, API data to your application\n",
    "2. **Coordination** - Develop reusable, modularized pipelines to execute complex workflows \n",
    "3. **Agency** - Enable your LLM to interact with it's environmetn via decision making"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of using the Framework\n",
    "1. Components - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
    "\n",
    "2. Customized Chains - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. Speed ðŸš¢ - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. Community ðŸ‘¥ - Wonderful discord and community support, meet ups, hackathons, etc.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting your model on AWS\n",
    "To work with your models on AWS you can use either an integration with the SageMaker endpoint, or in the future directly talk to the Bedrock API. \n",
    "\n",
    "For now, let's look at how to work with a custom SageMaker Model Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model endpoint: tiiuae/falcon-40b-instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "# Set model configuration\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"max_length\": 1024,\n",
    "    # \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    # \"top_p\": 0.50,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"return_full_text\": False,\n",
    "    \"include_prompt_in_result\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "# Instantiate all available models \n",
    "sm_llm = SagemakerEndpoint(\n",
    "endpoint_name=_MODEL_CONFIG_[inference_model]['endpoint_name'],\n",
    "region_name=aws_region,\n",
    "model_kwargs=parameters,\n",
    "content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(f\"Loaded model endpoint: {inference_model}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an proprietary API endpoint such as OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(success) - Successfully connected to OpenAI\n",
      "You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\n"
     ]
    }
   ],
   "source": [
    "# Work with OpenAI\n",
    "from langchain import OpenAI\n",
    "\n",
    "if openai_api_key:\n",
    "    openai_llm = OpenAI(openai_api_key=openai_api_key)\n",
    "    print(\"(success) - Successfully connected to OpenAI\")\n",
    "else:\n",
    "    print(\"(failure) - You have not provided an OpenAPI key, and you won't have access to work with the model in this notebook\")\n",
    "\n",
    "# Work with Anthropic\n",
    "from langchain import Anthropic\n",
    "\n",
    "if anthropic_api_key:\n",
    "    anthropic_llm = Anthropic(anthropic_api_key=anthropic_api_key)\n",
    "    print(\"(success) - Successfully connected to Anthropic\")\n",
    "else:\n",
    "    print(\"You have not provided an AnthropicAPI key, and you won't have access to work with the model in this notebook\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select your model\n",
    "To showcase you how different the models behave to prompting you can choose to select between an OpenSource Leaderboard model `Falcon-40B-Instruct Model` and `OpenAIs Davinci Model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853fd3cdf91a4285888ceab2ae2b1e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Select(options=('SageMaker-Falcon40B', 'OpenAI'), value='SageMaker-Falcon40B')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SageMaker-Falcon40B'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated OpenAI\n"
     ]
    }
   ],
   "source": [
    "match model_selection:\n",
    "    case \"SageMaker-Falcon40B\":\n",
    "        llm = sm_llm\n",
    "    case \"OpenAI\":\n",
    "        llm = openai_llm\n",
    "        \n",
    "print(f\"Activated {model_selection.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SageMaker-Falcon40B'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "Saturday\n"
     ]
    }
   ],
   "source": [
    "print(llm(\"What day comes after Friday\").strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a basic langchain application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LangChain application centers around your LLM model. This can be either a deployed inference endpoint, or a managed service (Bedrock, OpenAI API). The framework provides a series of out of the box integrations in the `llms` module and can be easily expanded to your use case. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "A model takes a series of messages and returns a message as output\n",
    "\n",
    "You can choose between:\n",
    "1. **LanguageModel** Takes text and returns text\n",
    "2. **Chat Model** Takes a series of messages and returns a message output\n",
    "3. **Embedding Models** Transform your text into a latent space vector to power similarity search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "A wrapper around a typical text input, text output interaction with the model. No structure is expected, and no structure is maintained. Good starting point for many non-chat applications. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we established our connection, we can query the model by sending it instructions as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Prompt Engineering Template Factory\n",
      "2. Prompt Engineering Template Library\n",
      "3. Prompt Engineering Template Generator\n",
      "4. Prompt Engineering Template Creator\n",
      "5. Prompt Engineering Template Builder\n",
      "6. Prompt Engineering Template Maker\n",
      "7. Prompt Engineering Template Designer\n",
      "8. Prompt Engineering Template Creator\n",
      "9. Prompt Engineering Template Generator\n",
      "10. Prompt Engineering Template Factory\n"
     ]
    }
   ],
   "source": [
    "text = \"Give me 10 names for a template factory library for prompt engineering. Ensure to create the required number of examples. Only provide the items of the list\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_prompt = \"\"\"\n",
    ">>QUESTION<<Create a list of services a company named {prompt} could sell.\n",
    ">>ANSWER<<\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Manufacturing services: FactoryBot could offer manufacturing services to other companies, such as producing parts or assembling products.\n",
      "\n",
      "2. Customization services: FactoryBot could offer customization services to other companies, such as adding logos or branding to products.\n",
      "\n",
      "3. Quality control services: FactoryBot could offer quality control services to other companies, such as testing products for defects or ensuring they meet industry standards.\n",
      "\n",
      "4. Logistics services: FactoryBot could offer logistics services to other companies, such as shipping products or managing inventory.\n",
      "\n",
      "5. Consulting services: FactoryBot could offer consulting services to other companies, such as advising on manufacturing processes or identifying areas for improvement.\n",
      "\n",
      "6. Training services: FactoryBot could offer training services to other companies, such as teaching employees how to use manufacturing equipment or implementing new processes.\n",
      "\n",
      "7. Maintenance services: FactoryBot could offer maintenance services to other companies, such as repairing equipment or performing regular maintenance checks.\n",
      "\n",
      "8. Research\n"
     ]
    }
   ],
   "source": [
    "print(llm(complex_prompt.format(prompt=\"FactoryBot\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use vanilla string formatting to integrate information into our models. This allows us to pass information in a structed manner into the model, masking the general nature of the model. This allows to create all the common products you see being built natively on LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect_prompt = \"\"\"\n",
    "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
    "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
    "focussed. Given the following context\n",
    "\n",
    "Context:\n",
    "#System Requirements:\n",
    "{requirements}\n",
    "#Scale:\n",
    "{scale}\n",
    "#Features:\n",
    "{features}\n",
    ">>QUESTION<<Describe an architecture on AWS in technical detail\n",
    ">>ANSWER<<\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "A website for my foodstore\n",
      "#Scale:\n",
      "Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\n",
      "#Features:\n",
      "Landing page describing our product. About page describing the company. Career page describing open positions.\n",
      ">>QUESTION<<Describe an architecture on AWS in technical detail\n",
      ">>ANSWER<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = architect_prompt.format(\n",
    "    requirements=\"A website for my foodstore\", \n",
    "    scale=\"Must handle 10k requests per second in peak. Must be globally available. Must be reponsive and fast\", \n",
    "    features=\"Landing page describing our product. About page describing the company. Career page describing open positions.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The solution architecture for the foodstore website would be as follows:\n",
      "\n",
      "1. Front-end: The front-end would be hosted on Amazon Elastic Compute Cloud (EC2) instances with Auto Scaling enabled to handle the peak traffic of 10k requests per second. The instances would be running Amazon Linux 2 and would be configured with Elastic Load Balancing (ELB) to distribute the traffic across multiple instances. The front-end would be built using a combination of Amazon Simple Storage Service (S3) and Amazon CloudFront to serve static content and Amazon Elastic Compute Cloud (EC2) instances to serve dynamic content.\n",
      "\n",
      "2. Back-end: The back-end would be hosted on Amazon Elastic Compute Cloud (EC2) instances with Auto Scaling enabled to handle the peak traffic of 10k requests per second. The instances would be running Amazon Linux 2 and would be configured with Elastic Load Balancing (ELB)\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but can get a bit clunky when you try to scale it out to more complex use cases. The next type of model wrapper provides a solution to this problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "These models structure their input and outputs with Schemata that enable you to reason about the expected input and output process. This helps to build more complex designs by seperating the inputs used to provide the model with its role instruction, the query and the context to the query. \n",
    "\n",
    "Currently this is only implemented for API based models such as ChatGTP and Anthropic.\n",
    "\n",
    "This section is OPTIONAL, as you will have to have your own ChatAntrophic API key to follow along. Currently registration for API keys is closed as they roll out the service. If you do not have a key yet, just read through the outputs of the notebook for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm =ChatOpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmm, let me think... Oh, I know! Have you tried walking all the way to New York? It's great exercise and you'll get to see some nice scenery along the way. Just be sure to pack some snacks and a good pair of shoes.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "response = chat_llm([\n",
    "    SystemMessage(content=\"You are an unhelpful AI bot that makes jokes at whatever the user says.\"),\n",
    "    HumanMessage(content=\"I would like to go to New York, how should i do this?\"),\n",
    "    AIMessage(content=\"???\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemata\n",
    "\n",
    "We see that ChatModels use typed classes to structure inputs. This is an example of a LangChain `Schema`, but its just one of many. \n",
    "\n",
    "LangChain currently provides the following schemata:\n",
    "\n",
    "* **Text** The primary interface to interact with a model (used with LanguageModels\n",
    "* **ChatMessages** What you saw we defined up with the ChatModel\n",
    "* **Examples** Input/output pairs acting as context for fine tuning model behavior in n-shot learning\n",
    "* **Document** Piece of unstructured data holding data as content and metadata for retrieval in context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatMessages Schema\n",
    "The primary interface through which end users interact with these is a chat interface. For this reason, some model providers even started providing access to the underlying API in a way that expects chat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "hum_msg = HumanMessage(content='inputs send to the model by the user', additional_kwargs={}, example=True)\n",
    "hum_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='Instructions to the model', additional_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = SystemMessage(content='Instructions to the model', additional_kwargs={})\n",
    "sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Context answer providing further input to the model', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg = AIMessage(content='Context answer providing further input to the model', additional_kwargs={})\n",
    "ai_msg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure allows us to simply pass multiple requests into a model for batch processing, making application integration easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple sets of messages\n",
    "batch_messages = [\n",
    "    [   SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to German.\"),\n",
    "        HumanMessage(content=\"What a wonderful day we had at the beach this late summer.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Documents\n",
    "A piece of unstructured data. Consists of page_content (the content of the data) and metadata (auxiliary pieces of information describing attributes of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add code on how to use Documents "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1\n",
    "We are working to enable our marketing team to provide customized sales emails at scale. You are asked to create to engineer a prompt for a custom marketing email copy creation pipeline. \n",
    "\n",
    "You will be given the following inputs that are collected on the users in your database:\n",
    "* Name \n",
    "* Age\n",
    "* Interest (List of strings)\n",
    "\n",
    "You will also be given a recommended product to personalize-recommend to the user\n",
    "* Product described as a dictionary of attributes (document from DB)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "#TODO Rewrite for callcenter use case\n",
    "\n",
    "# Complete the function \n",
    "def create_email_copy(name: str, age: int, interests: List[str], product: dict) -> str:\n",
    "    \"\"\"\n",
    "    The email should be personalized, be age appropriate, target the interests \n",
    "    of the person and market the product you are selling. \n",
    "\n",
    "    Fill in this template using string formatting and a combination of the prompt\n",
    "    engineering techniques you have learned previously. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the product you are selling. Play with the level of detail\n",
    "\n",
    "product = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of users to generate eamils for\n",
    "users = [\n",
    "    {\n",
    "    \"name\": \"\",\n",
    "    \"age\": 0,\n",
    "    \"intesrests\": [],\n",
    "    \"product\": product\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m user \u001b[39min\u001b[39;00m users:\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mprint\u001b[39m(llm(create_email_copy(user)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "# Test your marketing output\n",
    "for user in users:\n",
    "    print(\"\\n\\n\")\n",
    "    print(llm(create_email_copy(user)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building more complex scenarios, managing the parameters placed into the templates can be too complex for simple string injection methods. Eventually you want to describe the interface in a more programmatic way. Here the `PromptTemplate` helps to define verified input variables to be utilized in the format string.\n",
    "\n",
    "### The PromptTemplate class \n",
    "\n",
    "Let's structure our architecture template to make it reusable in our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# First we can define an exposed parameter interface to the format string\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"requirements\", \"scale\", \"features\"],\n",
    "    template=architect_prompt,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template can be asked to format itself, returning the compiled format string for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Play the role of a solution architect experienced with AWS. You are analysing customer requirements to create\n",
      "well-architected solution architectures that you present to the customer. You are detailled, kind and\n",
      "focussed. Given the following context\n",
      "\n",
      "Context:\n",
      "#System Requirements:\n",
      "External facing web application written in Javascript, global deployment\n",
      "#Scale:\n",
      "Average of 500 requests per minute, scale events up to 3000 requests per second\n",
      "#Features:\n",
      "Mobile website, desktop version, javascript\n",
      ">>QUESTION<<Describe an architecture on AWS in technical detail\n",
      ">>ANSWER<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = architect_prompt.format(\n",
    "    requirements=\"External facing web application written in Javascript, global deployment\",\n",
    "    scale=\"Average of 500 requests per minute, scale events up to 3000 requests per second\",\n",
    "    features=\"Mobile website, desktop version, javascript\"\n",
    ")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe proposed solution would be a serverless, scalable, and cost-effective architecture on AWS using several services such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Block Store (EBS), Amazon Simple Storage Service (S3), Amazon Elastic Load Balancer (ELB), Amazon CloudFront, and Amazon Route 53.\\n\\nThe web application would be hosted on EC2 instance(s) using a web application server like Apache or Nginx. The application code and resources would be stored on EBS volumes attached to the instance(s). S3 would be used to store static resources like images and videos.\\n\\nELB would distribute the incoming requests evenly across several instances, ensuring high availability and scalability. CloudFront would be used to cache the content closer to the end-users, reducing the load on the origin and improving the user experience.\\n\\nRoute 53 would be used to route traffic to the appropriate EC2 instances based on geographic location'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(final_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a string output is nice and dandy, but what if we want to create structure returns for further use in our applications. For example, how would we continue working with an extracted set of attributes from a text in a parser scenario? \n",
    "\n",
    "Is a string good enough, or would we rather want to return a named tuple, dict or list of class instances from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=\"\"\"\n",
    "    You identify named entities in the text and extract relations amongst them. \n",
    "    You do not answer questions, and you do not ask questions.\n",
    "    It is very important to extract all references you find. Do not skip any in your output.\n",
    "\n",
    "    # Examples:\n",
    "    The Dow Jones closed with a plus of 1456 points // (\"Dow Jones\", \"closed\", \"1456 points\")\n",
    "\n",
    "    Q: {text} // \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You identify named entities in the text and extract relations amongst them. \n",
      "    You do not answer questions, and you do not ask questions.\n",
      "    It is very important to extract all references you find. Do not skip any in your output.\n",
      "\n",
      "    # Examples:\n",
      "    The Dow Jones closed with a plus of 1456 points // (\"Dow Jones\", \"closed\", \"1456 points\")\n",
      "\n",
      "    Q: Mister Higgins bought the old house next to the woods. // \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(references.format(text=\"Mister Higgins bought the old house next to the woods.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Implement the ParserClass\n",
    "class ReferenceOutputParser(BaseOutputParser):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Example\n",
    "These can be inputs/outputs for a model or for a chain. Both types of examples serve a different purpose. Examples for a model can be used to finetune a model. Examples for a chain can be used to evaluate the end-to-end chain, or maybe even train a model to replace that whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples are structured as Q/A pairs. Let's create a list of fewshot examples for us to explore\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: How old was Muhammad Ali when he died?\n",
    "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "        Follow up: How old was Alan Turing when he died?\n",
    "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "        So the final answer is: Muhammad Ali\n",
    "    \"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "    \"\"\"\n",
    "        Are follow up questions needed here: Yes.\n",
    "        Follow up: Who was the founder of craigslist?\n",
    "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "        Follow up: When was Craig Newmark born?\n",
    "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "        So the final answer is: December 6, 1952\n",
    "    \"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use a formatter to parse the examples into string inputs to our template\n",
    "example_prompt = PromptTemplate(input_variables=['question', 'answer'], template=\"Question: {question}\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: How old was Muhammad Ali when he died?\n",
      "        Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "        Follow up: How old was Alan Turing when he died?\n",
      "        Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "        So the final answer is: Muhammad Ali\n",
      "    \n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "\n",
      "        Are follow up questions needed here: Yes.\n",
      "        Follow up: Who was the founder of craigslist?\n",
      "        Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "        Follow up: When was Craig Newmark born?\n",
      "        Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "        So the final answer is: December 6, 1952\n",
      "    \n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "# Feed the examples and formatter to FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=['input']\n",
    ")\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an example selector\n",
    "If you provide a full set of examples that cover various different topics in depth the lenght of the context can overrun the memory allocation of your model endpoint. \n",
    "\n",
    "Example selectors help to pass a subset of the examples that are relevant to the specific question at hand instead of passing the full examples.\n",
    "\n",
    "The example selector utilizes a similarity score across the embedded question and example pairs. We will cover embeddings in detail in Lab2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membeddings\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      6\u001b[0m example_selector \u001b[39m=\u001b[39m SemanticSimilarityExampleSelector\u001b[39m.\u001b[39mfrom_examples(\n\u001b[0;32m----> 7\u001b[0m     examples,\n\u001b[1;32m      8\u001b[0m     \u001b[39m#TODO: Replace with embedding model endpoint\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     OpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mopenai_api_key),\n\u001b[1;32m     10\u001b[0m     Chroma,\n\u001b[1;32m     11\u001b[0m     k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39m# Select the most similar example to the input.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho was the father of Mary Ball Washington?\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'examples' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    #TODO: Replace with embedding model endpoint\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "    Chroma,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Select the most similar example to the input.\n",
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"Examples most similar to the input: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchaining "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex workflows will involve the abbility of the model to react to specific subsets of tasks with specialized sequence of behaviors. We would capture these subsections of our application into modules called `chains`. \n",
    "\n",
    "Each chain structures the interaction with a model through specialized prompts, optional examples and optional output parsers. \n",
    "\n",
    "The langchain chains module contains the classes to help us easily create specialized sequences of chains that can receive inputs from previous model inferences as structured outputs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LLMChain\n",
    "\n",
    "Let's play through the example of creating a product that allows users to generate a full set of marketing materials.\n",
    "\n",
    "1. Generate name proposals for a company based on intended product to be sold\n",
    "2. Generate a marketing slogan based on company values provided\n",
    "3. Create a marketing template for email communication for the new company launch\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    System: You are a helpful marketing assistant that creates a marketable company name for a company selling. Answer with a single name only no comments or discussion.\n",
    "    Human: {product}\n",
    "    \"\"\"\n",
    ")\n",
    "company_name_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference endpoint: An error occurred (ExpiredTokenException) when calling the InvokeEndpoint operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/sagemaker_endpoint.py:236\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49minvoke_endpoint(\n\u001b[1;32m    237\u001b[0m         EndpointName\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[1;32m    238\u001b[0m         Body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    239\u001b[0m         ContentType\u001b[39m=\u001b[39;49mcontent_type,\n\u001b[1;32m    240\u001b[0m         Accept\u001b[39m=\u001b[39;49maccepts,\n\u001b[1;32m    241\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_endpoint_kwargs,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredTokenException) when calling the InvokeEndpoint operation: The security token included in the request is expired",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m company_name_chain\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mcolorful socks\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(result)\n\u001b[1;32m      3\u001b[0m \u001b[39m# print(company_name_chain.run(\"colorful socks\"))\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/chains/base.py:267\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/chains/base.py:149\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 149\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    151\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    152\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    153\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/chains/base.py:143\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    137\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    138\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    139\u001b[0m     inputs,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    144\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    145\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:74\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     72\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 74\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:84\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m     85\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     86\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:139\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    138\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:203\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    204\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:195\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    191\u001b[0m     dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    196\u001b[0m             prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    199\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/base.py:493\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    492\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 493\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    494\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    495\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m     )\n\u001b[1;32m    497\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    498\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Documents/Coding/ml/generativeai/gai-bootcamp/Day1/.venv/lib/python3.11/site-packages/langchain/llms/sagemaker_endpoint.py:244\u001b[0m, in \u001b[0;36mSagemakerEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39minvoke_endpoint(\n\u001b[1;32m    237\u001b[0m         EndpointName\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name,\n\u001b[1;32m    238\u001b[0m         Body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_endpoint_kwargs,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference endpoint: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    246\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_handler\u001b[39m.\u001b[39mtransform_output(response[\u001b[39m\"\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m stop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# This is a bit hacky, but I can't figure out a better way to enforce\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m# stop tokens when making calls to the sagemaker endpoint.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference endpoint: An error occurred (ExpiredTokenException) when calling the InvokeEndpoint operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "result = company_name_chain.run(\"colorful socks\")\n",
    "print(result)\n",
    "# print(company_name_chain.run(\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.trim()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain can generate batch predictions to answer multiple questions at a time using the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'product': \"Kids kites\"},\n",
    "    {'product': \"Running shoes\"},\n",
    "    {'product': \"Tennis sports wear\"},\n",
    "]\n",
    "\n",
    "res = company_name_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Kite Kids Inc.\n",
      "    - Fly High Kites\n",
      "    - Kite-Flyers Inc.\n",
      "    - Sky High Kites\n",
      "    - Rainbow Kites\n",
      "    - Kite Dreams\n",
      "    - Wind Riders\n",
      "    - Air Kites\n",
      "    - Kite Connection\n",
      "    - Kids Kites Co.\n",
      "    - Kite Masters\n",
      "    - Kite-o-rama\n",
      "    - Kite Factory\n",
      "    - Kite Zone\n",
      "    - Kid Kites\n",
      "- Runner's Paradise\n",
      "    - Foot Forward\n",
      "    - The Running Shoe Guru\n",
      "    - Run Fast, Run Free\n",
      "    - Sole Purpose\n",
      "    - The Running Shoe Store\n",
      "    - The Shoe Shack\n",
      "    - Sole Mate\n",
      "    - Run Happy\n",
      "    - Run Free\n",
      "    - The Running Company\n",
      "    - Fast Feet\n",
      "    - Shoe City\n",
      "    - Runner's Edge\n",
      "    - Shoe Depot\n",
      "    - Shoe Palace\n",
      "    - Run 4 Life\n",
      "    - Running Shoes for All\n",
      "    - Sole Train\n",
      "    - Running with Style\n",
      "    - The Shoe Spot\n",
      "    - Running Gear\n",
      "    - The Running Shoe Depot\n",
      "    - The Running Shoe Outlet\n",
      "    - The Running Shoe Connection\n",
      "    - The Shoe Box\n",
      "    - The Running Company\n",
      "\n",
      "I am sorry, but as an AI language model, I cannot provide a subjective opinion. However, I can suggest that you choose a name that resonates with your target audience and is unique to your brand. Good luck with your business!\n",
      "- Ace Tennis Wear\n",
      "    - Ace Tennis Apparel\n",
      "    - Tennis Ace\n",
      "    - Ace Sports Tennis\n",
      "    - Ace Sportswear\n",
      "    - Ace Sports Apparel\n",
      "    - Ace Sports Outfitters\n",
      "    - Ace Sports Gear\n",
      "    - Ace Sports Clothing\n",
      "    - Ace Sports Equipment\n",
      "    - Ace Sports Accessories\n",
      "    - Sports Ace\n",
      "    - Ace Sports Shop\n",
      "    - Ace Sports Shop\n"
     ]
    }
   ],
   "source": [
    "for response in res.generations:\n",
    "    print(response[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use a formatter to parse the examples into string inputs to our template\n",
    "example_prompt = PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a more realistic problem to solve with LLMs that we can not easily solve with a simngle prompt. Here we see the power of chaining a series of steps conducted by an agent using serialized IO of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"\"\"\n",
    "    \"\"\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: First create name of the company, then provide a slogan, then create a mission and vision statement\n",
    "\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=['product', 'company_name'],\n",
    "    template=\"\"\"\n",
    "    Context:\n",
    "    You are desigining a corporate identity for {company_name}.\n",
    "    The company is selling {product}. \n",
    "    Create a slogan for the company that is unique and memorable.\n",
    "    \"\"\"\n",
    ")\n",
    "# Slogan chain\n",
    "slogan_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=slogan_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSole Purpose: Run for Life\\n\\n<p>The slogan \"Run for Life\" captures the essence of the company\\'s mission, which is to encourage people to lead active, healthy lifestyles. It also emphasizes the longevity of their products, as they are designed to last for a long time. Additionally, the use of \"run\" in the slogan emphasizes the company\\'s focus on running shoes, making it clear to potential customers that this is the type of product they offer.</p>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slogan_chain.run(company_name=\"Sole Purpose\", product=\"running shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'company_name_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m SequentialChain\n\u001b[1;32m      3\u001b[0m marketing_chain \u001b[39m=\u001b[39m SequentialChain(\n\u001b[0;32m----> 4\u001b[0m     chains\u001b[39m=\u001b[39m[company_name_chain, slogan_chain],\n\u001b[1;32m      5\u001b[0m     input_variables\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mproduct\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     output_variables\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcompany_name\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mslogan\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'company_name_chain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "marketing_chain = SequentialChain(\n",
    "    chains=[company_name_chain, slogan_chain],\n",
    "    input_variables=['product'],\n",
    "    output_variables=['company_name', 'slogan']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "Let's bring it all together to create a custom LangChain that can build up a cooking article for our new online magazine. \n",
    "\n",
    "We will develop a series of chains to expand on a set of initial travel destinations to cover. \n",
    "\n",
    "Each article will contain:\n",
    "* A overview section of the travel destination\n",
    "* A list of things to do in the region\n",
    "* A recipe for a famous local dish (description, list of ingredients, step by step instruction)\n",
    "\n",
    "### Target\n",
    "Your system should create the articles based on the following inputs:\n",
    "\n",
    "* Travel destination (City, Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by creating the subchains\n",
    "\n",
    "# 1. Create the destination_overview chain\n",
    "\n",
    "\n",
    "# 2. Create the recommended_activities chain\n",
    "\n",
    "\n",
    "# 3. Create the famous_local_dish recommender \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the chains should be constructed for a subset of the workflow that is self contained. Extending too many tasks at once can overload the model and lead to poor results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now unifiy the elements and ensure the information flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of locations to cover in your reports\n",
    "location_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the chain against the location_list\n",
    "result = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We have seen how LangChain provides us with a series of abstractions on the core building blocks of LLM based applications. \n",
    "\n",
    "We have learned to connect our models, create custom Templates, use Schemata to structure our message flow, chain a series of steps and structure the models outputs to further pass into downstream systems. \n",
    "\n",
    "Each component of the system is under continued development and likely you will see the library change as it continues to mature. \n",
    "\n",
    "Still, we believe LangChain to be a useful abstraction to develop faster and build a more sustainable code base. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
